{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6478b4",
   "metadata": {},
   "source": [
    "# Sentiment analysis for Hindi/English code-mixed text.\n",
    "<hr/>\n",
    "\n",
    "### This file cleans the data, pre-processes it and creates a new csv that can be used to train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "115edf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the main imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "# All the nltk imports\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abfb5e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the appropriate dataset file\n",
    "df = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40e2cf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data distribution between different labels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "neutral     5264\n",
       "positive    4634\n",
       "negative    4102\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The data distribution between different labels: ')\n",
    "df['Label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0ce7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if the any null data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id     Sentence  Label\n",
       "False  False     False    3000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Check if the any null data')\n",
    "df.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f37b59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the null data\n",
    "df = df[df['Sentence'].isnull() == False]\n",
    "df = df[df['Label'].isnull() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d27b5a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id     Sentence  Label\n",
       "False  False     False    3000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate if all the null data is removed\n",
    "df.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c8318",
   "metadata": {},
   "source": [
    "### Removing all the languages except Hin & Eng as those token will not be useful in the training process\n",
    "\n",
    "Words with `O` Language tag are special characters\n",
    "Words with `EMT` Language tag are emojis\n",
    "Words with `positive, negative, neutral` Language tag are miss labels.\n",
    "\n",
    "Removing all these dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88cf64d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3000 entries, 0 to 2999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3000 non-null   int64 \n",
      " 1   Sentence  3000 non-null   object\n",
      " 2   Label     3000 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 93.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# The summary of dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5425bc41",
   "metadata": {},
   "source": [
    "### We will be performing following tasks on the data\n",
    "\n",
    "1. Make all the data to lower case\n",
    "2. Stemming the data\n",
    "3. Lemmatizing the data\n",
    "4. Removing stopwords\n",
    "5. Removing the usernames 'Words with digits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16bfc15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task1 : Turning the data to lower case\n",
    "df['Sentence'] = df['Sentence'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58020b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task2 : Stemming\n",
    "##  Define function to perform stemming on words\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# def perform_stemming(token):\n",
    "#     if token['Language'] == 'Hin':\n",
    "#         perform_hin_stemming(token['Words'])\n",
    "#     else:\n",
    "#         perform_eng_stemming(token['Words'])\n",
    "\n",
    "def perform_stemming(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "def perform_eng_stemming(word):\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    return stemmed_word\n",
    "\n",
    "def perform_hin_stemming(word):\n",
    "    stemmed_word = re.sub(r'(.{2,}?)([aeiougyn]+$)',r'\\1', word)\n",
    "    return stemmed_word\n",
    "\n",
    "## Perform stemming based on if the Language assigned in hindi or english\n",
    "# df['Words'] = df['Words'].apply(lambda token: perform_stemming(token), axis=1)\n",
    "df['Sentence'] = df['Sentence'].apply(perform_stemming)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b492f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 : lemmatization\n",
    "\n",
    "# Define function to perform lemmatization on words\n",
    "# def perform_lemmatization(word):\n",
    "#     lemmatized_words = lemmatizer.lemmatize(word, pos=wordnet.VERB)\n",
    "#     return lemmatized_words\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def perform_lemmatization(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(token, pos=wordnet.VERB) for token in tokens]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "\n",
    "df['Sentence'] = df['Sentence'].apply(perform_lemmatization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74545b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 : Remove stopwords\n",
    "\n",
    "stop_words_set_eng = set(stopwords.words('english'))\n",
    "stop_words_set_hin = set([\n",
    "    'is', 'ke', 'ka', 'ek', 'hai', 'hain', 'ki', 'ko', 'mein', 'se', 'par', 'bhi', 'ke', 'liye', 'saath',\n",
    "    'ho', 'kar', 'vale', 'vali', 'kuch', 'jo', 'to', 'hi', 'tak', 'ya', 'hote', 'hota', 'tha', 'the',\n",
    "    'ab', 'jab', 'kahaa', 'kisi', 'ne', 'unke', 'uske', 'uski', 'usmein', 'uskoe', 'usse', 'iskay',\n",
    "    'iski', 'ismein', 'iskoe', 'isse', 'tab', 'phir', 'jaise', 'jiske', 'jiskee', 'jismein', 'jiskoe',\n",
    "    'jisse', 'yah', 'yahee', 'ye', 'vah', 'vahee', 've', 'kai', 'kul', 'door', 'parantu', 'aap', 'tum',\n",
    "    'tumhara', 'tumhare', 'main', 'mera', 'mere', 'ham', 'hamara', 'hamare', 'apna', 'apne', 'khud',\n",
    "    'yahan', 'vahan', 'sabka', 'sabke', 'kisi', 'kise', 'sabhi', 'sab', 'koi', 'kuch', 'kisi',\n",
    "    'kisi', 'kisi', 'koi', 'dusra', 'any', 'any', 'aur', 'etc'\n",
    "])\n",
    "\n",
    "stop_words_set = stop_words_set_eng | stop_words_set_hin\n",
    "\n",
    "# def check_if_stopwords(word):\n",
    "#     return word not in stop_words_set\n",
    "\n",
    "\n",
    "# # df['Words'] = df['Words'].apply(lambda token: check_if_stopwords(token))\n",
    "# for i in stop_words_set:\n",
    "#     df = df[df['Words'] != i]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = word_tokenize(text)\n",
    "    for t in tokens:\n",
    "        if t not in stop_words_set:\n",
    "            filtered_tokens.append(t)\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "df['Sentence']= df['Sentence'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1da1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Remove usernames (Words with digits in it.)\n",
    "\n",
    "# df['alpha'] = df['Words'].str.contains(r'\\d', regex=True)\n",
    "# df = df[df['alpha'] != True]\n",
    "\n",
    "def remove_num(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = word_tokenize(text)\n",
    "    for t in tokens:\n",
    "        if re.search(r'\\d',t):\n",
    "            continue;\n",
    "        else:\n",
    "            filtered_tokens.append(t)\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def remove_space(tokens):\n",
    "    return tokens.strip()\n",
    "\n",
    "df['Sentence']= df['Sentence'].apply(remove_num)\n",
    "df['Sentence']= df['Sentence'].apply(remove_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b665eb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of          id                                           Sentence     Label\n",
       "0     30258  prahladspatel modi mantrimand may samil honay ...  positive\n",
       "1     16648  bkunalraj tajinderbagga jammupalchhin shehla r...  negative\n",
       "2     28511  waglenikhil u saw cast religion nation saw tal...  negative\n",
       "3     10466  delhipolic sir local polic station pe complain...   neutral\n",
       "4     19266  maahi song kesari current favourit music melod...  positive\n",
       "...     ...                                                ...       ...\n",
       "2995  16859  rt mukeshsharmamla khushi nahi nayi sarkaar aa...  negative\n",
       "2996   2294  music life thank chhote ustad salman ali post ...   neutral\n",
       "2997  29819  vicki gilmour hmmmm realli sam outlaw someth a...   neutral\n",
       "2998  34181  rssurjewala incindia gala faad nahi chillana c...  negative\n",
       "2999  36603  lerki allah swt beha may ku lati chor diffah r...   neutral\n",
       "\n",
       "[3000 rows x 3 columns]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a9c1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the extra columns\n",
    "df.to_csv('data/final_data/validate_data_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83395b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
